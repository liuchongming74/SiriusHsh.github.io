---
layout: post
title:  Ng机器学习--课程笔记（二）
categories: 学习笔记 机器学习
tags: 机器学习
author: HSH
mathjax: true
---

* content
{:toc}

# Liner regression with one variable
## 1.model representation
首先讲了一个关于房价估计的单变量线性回归问题。  
知识点：  
1.关于训练集中术语的描述

  * m:训练样本的数目
  * x:输入变量，通常叫做特征量
  * y:输出变量，或者目标量





2.该问题的逻辑过程  
如图所示：  
![lesson2-4.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson2-4.png)  
训练集作为输入，通过学习算法，输出h, 这里h表示hypothesis(假设)，是一个函数。  
该问题的h函数，输入为房子大小，输出为房子的价格。  
h是一个x到y的映射函数。

3.h的表示  
对于该问题，是一个简单的单变量线性回归问题，所以模型可以表示为:  
$h_θ(x) = θ_0 + θ_1(x)$， $θ_i$为模型参数




## 2.cost function
知识点：  
1.对于该线性回归问题，代价函数可以表示成：   
$J(θ_0, θ_1)=\frac{1}{2m}\sum_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})^2$  => 乘以1/2m是为了在数学含义层面有更好的表示  
目标就是使得该代价函数最小，$\begin{matrix}minimize \\θ_0, θ_1 \end{matrix} J(θ_0, θ_1)$  
代价函数有时也被称为平方误差代价函数。  
2.需要理解的是假设函数$h_θ(x)$是对于一个**固定θ值，关于x的函数** ,而$J(θ)$是一个**关于θ的函数**  
假设$θ_0=0$，假设函数和代价函数的关系就下图所示（Ng在课程中画的）：  
![lesson2-5.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson2-5.png)  
由于我们的目标就是寻求使得代价函数$J(θ_1)$最小的$θ_1$值，从右图可以得到，当$θ_1$等于1是，代价函数最小。  

3.之间假设$θ_0=0$，当没有这个假设，存在两个参数时，代价函数描述如下：  
![lesson2-6.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson2-6.png)  
这是用MATLAB画出来的代价函数的3D图  
Ng讲课时用椭圆图来进行讲述，如下右图所示：  
![lesson2-7.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson2-7.png)  
一个椭圆上的点都是拥有相同的J值。  

## 3.Gradient descent
在上一小节，我们学习到了代价函数的表示方法并通过一些实际数据来形式化的描述出了代价函数的样子，当然，我们的目的并不是要画出代价函数的图像，然后人工地从图中寻找使得代价函数最小的$(θ_0,θ_1)$，而是希望通过编程自动地获取$(θ_0,θ_1)$值。    
如图所示  ![lesson2-8.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson2-8.png)  
梯度下降算法的实现过程，就犹如下山，假设一个起始点，然后在每一步上抉择下山最快的方向，每次迈出的步子大小（小碎步还是大步子），就是之后提到的学习率（α）决定。可以看到，当初始点选择不同时，最后"引导"到的目的地也不同。  

梯度下降算法描述：  
![lesson2-9.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson2-9.png)  
这里需要注意的是j值有两个，即0和1，所以等于也会有两个，梯度下降算法要求等式需要同步更新，即下图所示：  
![lesson2-10.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson2-10.png)  
对于梯度下降算法的一些知识点：  

* 当学习速率过小时，下降速度会很慢很慢
* 当学习速度过大时，可能会造成不收敛
* 即使是固定的学习速率，梯度下降算法也会自动地逐渐采用较小的幅度，因为越是接近局部最优解，导数值越小。因此没有必要另外减少α

## 4.Gradient descent for liner regression
将代价函数带入到梯度下降算法当中，得到  
![lesson2-11.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson2-11.png)  
该公式的计算用到了些微积分的知识，还算简单。


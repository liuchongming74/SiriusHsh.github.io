---
layout: post
title:  Ng机器学习课程笔记（三）
categories: 学习笔记
tags: 机器学习
author: HSH
mathjax: true
---

* content
{:toc}

# Linear Regression with multiple variables
上一节讲了单变量的线性回归问题，这一节针对更复杂的情况。  





## 1.multivariate linear regression（多元线性回归）
![lesson3-1.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson3-1.png)  
![lesson3-2.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson3-2.png)  
假设函数表示为：$h_θ(x)=θ_0+θ_1x_1+θ_2x_2+θ_3x_3+...+θ_nx_n$  
通产默认x0等于0  
于是  
$x=\begin{bmatrix}
x_0\\
x_1\\
...\\
x_n
\end{bmatrix}
θ=\begin{bmatrix}
θ_0\\
θ_1\\
...\\
θ_n
\end{bmatrix}$

$h_θ(x)=θ_0x_0+θ_1x_1+θ_2x_2+θ_3x_3+...+θ_nx_n=θ^Tx$

$J(θ)=\frac{1}{2m}\sum_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})$   
在进行计算时，善用矩阵运算能够加快运算速度，如损失函数就可以表示为  
![lesson3-8.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson3-8.png)  
结果与上式相同。

## 2.Gradient Descent
上一节学到了单元线性回归问题的梯度下降算法，如图：  
![lesson2-11.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson2-11.png)  
多元回归问题的梯度下降算法与之类似，  
![lesson3-3.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson3-3.png)  
可以简写为：  
![lesson3-4.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson3-4.png)  

## 3.feature scaling（特征缩放）
通过特征缩放能够使得梯度下降算法获得更好的效果，梯度下降的较快，特征缩放没有严格的要求，通常只要在相似的范围内就行。  
如−1 ≤ x(i) ≤ 1或者 −0.5 ≤ x(i) ≤ 0.5。Ng提出他认为−3 ≤ x(i) ≤ 3是一个可接受的范围。  
**mean normalization**均值标准化  
$x_i:=\frac{x_i−μ_i}{s_i}$  
μi表示i特征的平均值  
si表示i特征取值的最大值减去最小值,values(max-min)  

**在做实验的时候注意到，在对数据集做特征缩放的时候，需要将μi和si值保存下来。处理新数据时，使用同样的μi和si值进行缩放**  
## 4.学习率α
![lesson3-5.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson3-5.png)    
如果代价函数变化趋势如这些图所示，应该采用更小的α取值  

## 5.features and polynomial regression 特征选择与多项式回归
假设函数为$h_θ(x)=θ_0+θ_1x_1+θ_2x_2$,  
用该式子去代表$h_θ(x)=θ_0+θ_1(size)+θ_2(size)^2$  
size返回0~1000，于是需要使用特征缩放，使得x1,x2在相同的范围内。  

## 6.normal equation（正规方程）
相比梯度下降求解θ值的方法，这里介绍第二种方法：normal equation，正规方程法。  
对于数据集![lesson3-6.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson3-6.png)  

则，通过正则方程，得到θ值：  
$θ=(X^TX)^{-1}X^Ty$  
**证明：待补充**  
正则方程法不需要使用特征缩放  
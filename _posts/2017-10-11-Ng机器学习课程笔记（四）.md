---
layout: post
title:  Ng机器学习课程笔记（四）
categories: 学习笔记
tags: 机器学习
author: HSH
mathjax: true
---

* content
{:toc}

这一节开始讲分类问题，首先对分类问题的假设函数$h_\theta(x)$进行了定义，当仍然使用线性回归（linear regression）时，随着新样本的引入（即使该样本显而易见的属于某一分类），会对原假设函数产生影响（导致直线斜率改变）。随之引入了逻辑回归的概念。





## 1.Logistic Regression（逻辑回归）
逻辑函数借助了一种称为Sigmoid function 或 Logistic function的函数，对线性回归的假设函数稍作修改，逻辑函数的假设函数表示为：$h_\theta(x)=g(\theta^Tx)$，其中$g(z)=\frac{1}{1+e^{-z}}$。  
g(z)的函数曲线如图所示：  
![lesson4-1.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson4-1.png)  
可以看到sigmoid函数具有很明显的特点，当z>=0时，g(z)>=0;当z<0时，g(z)<0。很适合分类问题，无论样本取值是多少，最后假设函数h的范围都在0~1之间。  
总结逻辑回归的假设函数设定如下：  

* 当$\theta^Tx \geq 0，也就是 h_\theta(x) \geq 0.5$时，此时 y=1 （二分类问题，y=0或y=1）
* 当$\theta^Tx < 0，也就是 h_\theta(x) < 0.5$时，此时 y=0

$h_\theta(x)=P(y=1|x,\theta)$
## 2.Decision boundary 决策边界
Ng多次强调：决策边界不是数据集的特征，而是假设函数的一个属性，数据集对假设函数的$\theta$值做出引导，导出最终的$\theta$值，从而生成决策边界。  
决策边界说白了就是将数据集分为两部分的那个分类线（二分类问题）。  
决策边界的求法很简单，就是利用sigmoid函数的特性，令$\theta^Tx=0$，通过一些方法（目前还没讲到）求解出$\theta$的值，得到的函数就是决策边界。  

## 3.Cost function（损失函数）
损失函数:

>$J(\theta)=\frac{1}{m} \displaystyle \sum_{i=1}^{m}Cost(h_\theta(x^{(i)}),y^{(i)})$  
>$Cost(h_\theta(x),y)=-log(h_\theta(x))$  if y = 1  
>$Cost(h_\theta(x),y)=-log(1-h_\theta(x))$  if y = 0  
>两个式子可以合并为一个：$Cost(h_\theta(x),y)=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))$

**重点：向量表示形式**

$h=g(X\theta)$  
$J(\theta)=-\frac {1}{m}(y^Tlog(h)+(1-y)^Tlog(1-h))$

$J(\theta)和h_\theta(x)的曲线图如下所示：$  
![lesson4-2.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson4-2.png)
## 4.Gradient Descent（梯度下降）
逻辑回归的梯度下降算法
![lesson4-3.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson4-3.png)
向量表示形式：  
$\theta := \theta - \frac {\alpha}{m}X^T(g(X\theta)-y)$

## 5.Optimization algorithm（优化算法）
这里讲了高级优化算法，如“BFGS”，“L-GFGS”等，与梯度下降算法相比它们的计算速度更快。  
然后Ng介绍了如何在octave中调用这些库函数。  
首先定义一个函数，它能返回$J(\theta)和\frac {∂}{∂\theta_j}J(\theta)$  
格式如下：  
```matlab
function [jVal, gradient] = costFunction(theta)
  jVal = [...code to compute J(theta)...];
  gradient = [...code to compute derivative of J(theta)...];
end
```

然后使用octave提供的fminunc优化算法。  
```
options = optimset('GradObj', 'on', 'MaxIter', 100); 
initialTheta = zeros(2,1);
   [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);
```
*需要注意的是代码里各种index都是从1开始，octave的规范，和python从0开始的不一样*
## 6.Multiclass Classification：one-vs-all（一对多分类算法）
**利用逻辑回归解决多分类别分类问题**
![lesson4-4.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson4-4.png)  
如将邮件类型分为:
work(y=0)  
friend(y=1)  
family(y=2)  
设计三个分类器，[(y=0),(y=1,y=2)],[(y=1),(y=0,y=2)],[(y=2),(y=0,y=1)]
![lesson4-5.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson4-5.png)  

如图，对于K分类问题，就是设计K个假设函数，得出该类别的概率，概率最大的就是结果。

## 7.over-fitting
如图，左边为欠拟合（underfitting）也称为高偏差（high bias），中间是对的，左边为过拟合（overfitting）也称为高方差（high variance）  
解决过拟合的方法：  
1.减少特征的数量  
2.正则化  
什么是正则化，怎么使用在后面讲

## 8.regularization
从损失函数入手，设假设函数为为$θ_0+θ_1x+θ_2x_2+θ_3x_3+θ_4x_4$，损失函数为$min_\theta \frac{1}{2m} \displaystyle \sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2$。  
通过在损失函数加上$1000⋅θ_3^2+1000⋅θ_4^2$两项，就能减少$\theta_3和\theta_4$的权重，由于$\lambda$设置的比较大，1000已经能使得$\theta_3和\theta_4$值趋近于0。于是就起到了减少特征数的作用，使得原来过拟合的曲线变得更加顺滑。  
一种通用的方法是正则化所有的参数：  
$min_\theta \frac{1}{2m} [\displaystyle \sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda \displaystyle \sum_{j=1}^{n}\theta_j^2]$  
这里有两个注意点：  
1.附加项中j是从1开始，这意味着$\theta_0$是不受影响的。  
2.$\lambda$取值太大会造成欠拟合。  

$\theta$称为惩罚参数  

### Regularized Linear Regression
$J(\theta) =  \frac{1}{2m} [\displaystyle \sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda \displaystyle \sum_{j=1}^{n}\theta_j^2]$     
$min_\thetaJ(\theta)$  

1. 梯度下降算法
![lesson4-6.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson4-6.png)  
第二项可以变形为：$\theta_j := \theta_j(1-\alpha \frac{\lambda}{m}) - \alpha \frac{1}{m} \displaystyle \sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2$  
$1-\alpha \frac{\lambda}{m}$是一个小于1的数，也就是相当于在每次减小了点惩罚参数。  
2. 正规方程
![lesson4-7.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson4-7.png)

### Regularized Logistic Regression
损失函数：  
![lesson4-8.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson4-8.png)  
正则化后：  
![lesson4-9.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson4-9.png)  
加上了$\frac {\lambda}{2m} \displaystyle \sum_{j=1}^{n}\theta_j^2$

1. 梯度下降算法
![lesson4-6.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson4-6.png)  
梯度下降算法与线性回归是类似的，区别仅在于假设函数$h(\theta)$  
2. advanced optimization
![lesson4-10.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson4-10.png)
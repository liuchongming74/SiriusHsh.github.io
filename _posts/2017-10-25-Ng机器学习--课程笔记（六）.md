---
layout: post
title:  Ng机器学习--课程笔记（六）
categories: 学习笔记 机器学习
tags: 机器学习
author: HSH
mathjax: true
---

* content
{:toc}






# Neural Networks
## 1.Cost Function
![lesson6-1.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson6-1.png)  
对网络中的一些参数说明：  
L:网络总层数  
$s_l$:l层神经元数（不包括偏置元 bias unit）  
K:输出神经元数  
复习一下，逻辑回归的损失函数：  
$J(\theta) = - \frac {1}{m} \displaystyle \sum_{i=1}^{m} [y^{(i)} * log(h_\theta(x^{(i)})) + (1- y^{(i)}) * log(1- h_\theta(x^{(i)}))] + \frac {\lambda}{2m} \displaystyle \sum_{j=1}^{n}\theta_j^2$   
对于神经网络，其损失函数更为复杂一点：  
$J(\theta) = - \frac {1}{m} \displaystyle \sum_{i=1}^{m} \displaystyle \sum_{k=1}^K[y_k^{(i)} * log((h_\theta(x^{(i)}))_k) + (1- y_k^{(i)}) * log(1- (h_\theta(x^{(i)}))_k)] + \frac {\lambda}{2m} \displaystyle \sum_{l=1}^{L-1} \displaystyle \sum_{i=1}^{s_l}  \displaystyle \sum_{j=1}^{s_{l+1}} (\theta_{j,i}^{(l)})^2$  
注：  i,j都是从1开始，因为index等于0的项不参与到正则化中。  
## 2.back propagation（反向传播算法）
反向传播算法是应用于神经网络中，用于降低损失函数；类似于应用在线性回归和逻辑回归的梯度下降算法。  
BP算法流程：  
![lesson6-2.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson6-2.png)  


>![lesson6-3.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson6-3.png)这张图还不太理解


反向传播算法，故名意思，计算$\delta$是倒着来的。   
![lesson6-4.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson6-4.png)对于如图所示的网络，
最后一层的$\delta$很好计算，$\delta_1^{(4)} = y^{(i)} - a_1^{(4)}$。  

待求的$\delta_2^{(2)} = \theta^{(2)}_{12}*\delta^{(3)}_1+\theta^{(2)}_{22}*\delta^{(3)}_2$   

同理可以求出$\delta^{(3)}_1 = \theta_{11}^{(3)}*\delta^{(4)}_1$和$\delta^{(3)}_2 = \theta^{(3)}_{12}*\delta^{(4)}_1$

## 3.BP算法的实现  
使用最优化算法步骤和逻辑回归、线性回归一样，都是先定义损失函数，然后利用fminunc函数进行最优化。  
![lesson6-5.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson6-5.png)  
有一点不同，thetaVec是所有$\theta$的总和，所以需要在函数内部进行还原。 
转换过程：  
```matlab
thetaVector = [Theta1(:); Theta2(:); Theta3(:); ]
deltaVector = [D1(:); D2(:); D3(:); ]
```

```matlba
%suppose Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11
Theta1 = reshape(thetaVector(1:110),10,11)
Theta2 = reshape(thetaVector(111:220),10,11)
Theta3 = reshape(thetaVector(221:231),1,11)
```

## 4.Gradient Checking（梯度检验）

梯度检验由于确保BP算法准确的工作了。  
BP算法得到的Dvec与梯度检验算法得到的gradApprox进行比较  
通过如下近似的计算方法：  
![lesson6-7.png](i`http://octtw77pk.bkt.clouddn.com//public/upload/lesson6-7.png)  
在octave中代码实现：  

```matlab
epsilon = 1e-4;
for i = 1:n,
  thetaPlus = theta;
  thetaPlus(i) += epsilon;
  thetaMinus = theta;
  thetaMinus(i) -= epsilon;
  gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon)
end;
```

确保gradApprox ≈ deltaVector  

## 5.Random Initialization（随机初始化）
对于神经网络，theta不能默认为0，否则会出现叫做对称现象的状况（Symmetry breaking).  
octave实现代码如下：   

```matlab
%If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11.

Theta1 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;
Theta2 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;
Theta3 = rand(1,11) * (2 * INIT_EPSILON) - INIT_EPSILON;

```

## 6.总结
![lesson6-8.png](http://octtw77pk.bkt.clouddn.com//public/upload/lesson6-8.png)